# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04b_nrt.ipynb (unless otherwise specified).

__all__ = ['RunManager']

# Cell
import pandas as pd
import scipy.io as sio
import requests
import IPython
import matplotlib.pyplot as plt
from nbdev.imports import test_eq
import datetime
from geoget.download import run_all
from .core import filter_files, ls, Path, InOutPath, ProjectPath
from .geo import Region
from .data import *
from .predict import predict_nrt
Path.ls = ls

# Cell
class RunManager():
    def __init__(self, project_path:ProjectPath, region, time='today',
                 product:str='VIIRS750', days=64, max_size=2000):
        self.path    = project_path
        self.time    = self.init_time(time)
        self.product = product
        self.region  = region
        self.days    = days
        self.max_size= 2000

    def init_time(self, time):
        if time == 'today':
            time = pd.Timestamp(datetime.date.today())
        elif time == 'yesterday':
            time = pd.Timestamp(datetime.date.today())-pd.Timedelta(days=1)
        return time

    def last_n_days(self, time:pd.Timestamp, days):
        return pd.date_range(start=time-pd.Timedelta(days=days-1), periods=days,
                              freq='D')

    def check_data(self):
        "Check existing and missing files in dataset folder."
        times = self.last_n_days(self.time, self.days)
        files, missing_files = [], []
        for t in times:
            tstr = t.strftime('%Y%m%d')
            file = self.path.dataset/f'{self.product}{self.region}_{tstr}.nc'
            if file.is_file():
                files.append(file)
            else:
                missing_files.append(file)
        return {'files': files, 'missing_files': missing_files}

    def get_download_dates(self):
        "Find for which new dates the files need to be downloaded."
        files = self.check_data()['files']
        if len(files) == 0:
            start = self.last_n_days(self.time, self.days)[0]
        else:
            start = pd.Timestamp(files[-1].stem.split('_')[-1])+pd.Timedelta(days=1)
        start = start.strftime('%Y-%m-%d 00:00:00')
        end = self.time.strftime('%Y-%m-%d 23:59:59')
        return start, end

    def update_hotspots(self, location, mode='7d', save=True):
        """Update hotspots file with new data.
          location is according to the data url naming format
          mode can be on of: 24h, 48h, 7d"""
        url = f'https://firms.modaps.eosdis.nasa.gov/' \
                   f'active_fire/viirs/text/VNP14IMGTDL_NRT_{location}_{mode}.csv'
        files = self.path.hotspots.ls(include=['.csv', f'hotspots{self.region}'])
        frp = [pd.read_csv(f) for f in files]
        frp = pd.concat([*frp, pd.read_csv(url)], axis=0, sort=False
                        ).drop_duplicates().reset_index(drop=True)
        if save:
            frp.to_csv(self.path.hotspots/f'hotspots{self.region}.csv', index=False)
            print(f'hotspots{self.region}.csv updated')
        else: return frp

    def download_viirs(self):
        "Download viirs data needed for the dataset."
        tstart, tend = self.get_download_dates()
        region = Region.load(f'{self.path.config}/R_{self.region}.json')

        if self.product == 'VIIRS750':
            viirs_downloader = VIIRS750_download(region, tstart, tend)
            viirs_downloader_list = viirs_downloader.split_times()

        elif self.product == 'VIIRS375':
            viirs_downloader1 = VIIRS375_download(region, tstart, tend)
            viirs_downloader2 = VIIRS750_download(region, tstart, tend,
                                bands=['SolarZenithAngle', 'SatelliteZenithAngle'])
            viirs_downloader_list1 = viirs_downloader1.split_times()
            viirs_downloader_list2 = viirs_downloader2.split_times()
            viirs_downloader_list = [*viirs_downloader_list1, *viirs_downloader_list2]

        else: raise NotImplementedError(f'Not implemented for {self.product}.')

        run_all(viirs_downloader_list, self.path.ladsweb)

    def preprocess_dataset_750(self):
        "Apply pre-processing to the rawdata and saves results in dataset directory."
        paths = InOutPath(f'{self.path.ladsweb}', f'{self.path.dataset}')
        R = Region.load(f'{self.path.config}/R_{self.region}.json')
        bands = ['Reflectance_M5', 'Reflectance_M7', 'Reflectance_M10', 'Radiance_M12',
                 'Radiance_M15', 'SolarZenithAngle', 'SatelliteZenithAngle']
        print('\nPre-processing data...')
        viirs = Viirs750Dataset(paths, R, bands=bands)
        merge_tiles = MergeTiles('SatelliteZenithAngle')
        mir_calc = MirCalc('SolarZenithAngle', 'Radiance_M12', 'Radiance_M15')
        rename = BandsRename(['Reflectance_M5', 'Reflectance_M7'], ['Red', 'NIR'])
        bfilter = BandsFilter(['Red', 'NIR', 'MIR'])
        act_fires = ActiveFires(f'{self.path.hotspots}/hotspots{self.region}.csv')
        viirs.process_all(proc_funcs=[merge_tiles, mir_calc, rename, bfilter, act_fires])

    def preprocess_dataset_375(self):
        "Apply pre-processing to the rawdata and saves results in dataset directory."
        paths = InOutPath(f'{self.path.ladsweb}', f'{self.path.dataset}')
        R = Region.load(f'{self.path.config}/R_{self.region}.json')
        bands = ['Reflectance_I1', 'Reflectance_I2', 'Reflectance_I3',
                 'Radiance_I4', 'Radiance_I5', 'SolarZenithAngle', 'SatelliteZenithAngle']
        print('\nPre-processing data...')
        viirs = Viirs375Dataset(paths, R, bands=bands)
        merge_tiles = MergeTiles('SatelliteZenithAngle')
        mir_calc = MirCalc('SolarZenithAngle', 'Radiance_I4', 'Radiance_I5')
        rename = BandsRename(['Reflectance_I1', 'Reflectance_I2'], ['Red', 'NIR'])
        bfilter = BandsFilter(['Red', 'NIR', 'MIR'])
        act_fires = ActiveFires(f'{self.path.hotspots}/hotspots{self.region}.csv')
        viirs.process_all(proc_funcs=[merge_tiles, mir_calc, rename, bfilter, act_fires])

    def preprocess_dataset(self):
        if self.product == 'VIIRS750':
            self.preprocess_dataset_750()
        elif self.product == 'VIIRS375':
            self.preprocess_dataset_375()
        else: raise NotImplementedError(f'Not implemented for {self.product}.')

    def init_model_weights(self, weight_files:list):
        "Downloads model weights if they don't exist yet on config directory."
        local_files = []
        for w in weight_files:
            file_save = self.path.config/w
            if not file_save.is_file():
                print(f'Downloading model weights {w}')
                url = f'https://github.com/mnpinto/banet_weights/raw/master/model/{w}'
                file = requests.get(url)
                open(str(file_save), 'wb').write(file.content)
            local_files.append(file_save)
        return local_files

    def get_preds(self, weight_files:list, threshold=0.5, save=True, max_size=2000):
        "Computes BA-Net predictions ensembling the models in the weight_files list."
        local_files = self.init_model_weights(weight_files)
        iop = InOutPath(self.path.dataset, self.path.outputs, mkdir=False)
        region = Region.load(f'{self.path.config}/R_{self.region}.json')
        predict_nrt(iop, self.time, local_files, region, threshold=threshold,
                    save=save, max_size=max_size, product=self.product)