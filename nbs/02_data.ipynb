{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from rasterio.coords import disjoint_bounds\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from functools import partial\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "from pyhdf.SD import SD, SDC\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from warnings import warn\n",
    "from nbdev.imports import test_eq\n",
    "from pyresample.geometry import SwathDefinition, AreaDefinition, CRS, create_area_def\n",
    "from pyresample import kd_tree\n",
    "\n",
    "from geoget.download import *\n",
    "from banet.core import *\n",
    "from banet.geo import *\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "> This module includes classes to create the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VIIRS375_download(Ladsweb):\n",
    "    \"Utility for downloading VIIRS 375m data to create the dataset.\"\n",
    "    def __init__(self, region, tstart, tend, bands=None):\n",
    "        product = 'NPP_VIAES_L1'\n",
    "        collection = '5000'\n",
    "        if bands is None:\n",
    "            bands = ['Reflectance_I1', 'Reflectance_I2', 'Reflectance_I3',\n",
    "                      'Radiance_I4', 'Radiance_I5']\n",
    "        super().__init__(product, collection, tstart, tend, list(region.bbox),\n",
    "                         bands, daynight='D', repPixSize=region.pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VIIRS750_download(Ladsweb):\n",
    "    \"Utility for downloading VIIRS 750m data to create the dataset.\"\n",
    "    def __init__(self, region, tstart, tend, bands=None):\n",
    "        product = 'NPP_VMAES_L1'\n",
    "        collection = '5000'\n",
    "        if bands is None:\n",
    "            bands = ['Reflectance_M5', 'Reflectance_M7', 'Reflectance_M10',\n",
    "                      'Radiance_M12', 'Radiance_M15', 'SolarZenithAngle',\n",
    "                      'SatelliteZenithAngle']\n",
    "        super().__init__(product, collection, tstart, tend, list(region.bbox),\n",
    "                         bands, daynight='D', repPixSize=region.pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BandsFilter():\n",
    "    \"\"\"Remove bands not in to_keep list from the dictionary.\"\"\"\n",
    "    def __init__(self, to_keep: list):\n",
    "        self.to_keep = to_keep if isinstance(to_keep, list) else [to_keep]\n",
    "\n",
    "    def __call__(self, data:dict, *args, **kwargs) -> dict:\n",
    "        keys = [k for k in data]\n",
    "        for k in keys:\n",
    "            if k not in self.to_keep:\n",
    "                del data[k]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BandsRename():\n",
    "    def __init__(self, input_names:list, output_names:list):\n",
    "        self.input_names = input_names if isinstance(input_names, list) else [input_names]\n",
    "        self.output_names = output_names if isinstance(output_names, list) else [output_names]\n",
    "\n",
    "    def __call__(self, data:dict, *args, **kwargs) -> dict:\n",
    "        for i, o in zip(self.input_names, self.output_names):\n",
    "            data[o] = data.pop(i)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MergeTiles():\n",
    "    def __init__(self, band:str, ignore:list=[]):\n",
    "        self.band = band\n",
    "        self.ignore = ignore\n",
    "\n",
    "    def __call__(self, data:dict, *args, **kwargs) -> dict:\n",
    "        d = np.nanmean(np.array(data[self.band]), axis=(1,2))\n",
    "        d = np.array(np.array(d).argsort())\n",
    "        masks = np.array(data[self.band])[d]\n",
    "        for k in data:\n",
    "            if k not in self.ignore:\n",
    "                data_aux = np.zeros_like(data[k][0])*np.nan\n",
    "                for dband, mask in zip(np.array(data[k])[d], masks):\n",
    "                    I = (np.isnan(data_aux)) & (~np.isnan(mask))\n",
    "                    data_aux[I] = dband[I]\n",
    "                data[k] = data_aux\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BandsAssertShape():\n",
    "    def __call__(self, data:dict, max_size=None, *args, **kwargs) -> dict:\n",
    "        for k in kwargs['cls'].bands:\n",
    "            rshape = kwargs['cls'].region.shape\n",
    "            if isinstance(data[k], list):\n",
    "                for d in data[k]:\n",
    "                    shape = d.shape\n",
    "                    if len(shape) == 3: # first is time\n",
    "                        shape = shape[1:]\n",
    "                    if shape != rshape:\n",
    "                        error = f'{k} shape {shape} does not match region shape {rshape}'\n",
    "                        raise Exception(error)\n",
    "            else:\n",
    "                shape = data[k].shape\n",
    "                if len(shape) == 3: # first is time\n",
    "                    shape = shape[1:]\n",
    "                if shape != rshape:\n",
    "                    error = f'{k} shape {shape} does not match region shape {rshape}'\n",
    "                    raise Exception(error)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ActiveFiresLog():\n",
    "    \"\"\"Get active fires, interpolate to grid and apply log1p.\"\"\"\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.lon = None\n",
    "        self.lat = None\n",
    "        self.df = self.load_csv()\n",
    "\n",
    "    def load_csv(self):\n",
    "        return pd.read_csv(self.file, parse_dates=['acq_date']).set_index('acq_date')\n",
    "\n",
    "    def __call__(self, data, time, *args, **kwargs):\n",
    "        R = kwargs['cls'].region.new()\n",
    "        if \"R\" in data and isinstance(data[\"R\"], Region):\n",
    "            R = data[\"R\"]\n",
    "        if R.pixel_size < 0.01: \n",
    "            Rbase = R.new(pixel_size=0.01)\n",
    "            self.lon, self.lat = Rbase.coords()\n",
    "        else:\n",
    "            self.lon, self.lat = R.coords()\n",
    "        frp = self.df[self.df.index == time]\n",
    "\n",
    "        if len(frp) > 0:\n",
    "            geometry = [Point(xy) for xy in zip(frp['longitude'], frp['latitude'])]\n",
    "            frp = GeoDataFrame(frp, geometry=geometry)\n",
    "            out = rasterize(frp, 'frp', Rbase, merge_alg='add')\n",
    "            if R.pixel_size < 0.01: \n",
    "                out = downsample(out, src_tfm=Rbase.transform, dst_tfm=R.transform,\n",
    "                                 dst_shape=R.shape, resampling='bilinear')\n",
    "            out[out==0] = np.nan\n",
    "        else: out = np.zeros(R.shape)*np.nan\n",
    "        data['FRP'] = np.log1p(out)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class InterpolateAngles():\n",
    "    \"\"\"Interpolated Angles to working resolution.\"\"\"\n",
    "    def __init__(self, R_base:Region, R_interp:Region, bands:list):\n",
    "        self.R_base   = R_base\n",
    "        self.R_interp = R_interp\n",
    "        self.bands    = bands\n",
    "    \n",
    "    def __call__(self, data:dict, *args, **kwargs):\n",
    "        R_interp = self.R_interp\n",
    "        if \"R\" in data and isinstance(data[\"R\"], Region): \n",
    "            R_interp = data[\"R\"]\n",
    "        for b in self.bands:\n",
    "            data[b] = [downsample(data[b][i], src_tfm=self.R_base.transform, \n",
    "                                 dst_tfm=R_interp.transform,\n",
    "                                 dst_shape=R_interp.shape, \n",
    "                                 resampling='bilinear')\n",
    "                       for i in range(len(data[b]))]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MirCalc():\n",
    "    def __init__(self, solar_zenith_angle:str, mir_radiance:str, tir_radiance:str,\n",
    "                 output_name:str='MIR'):\n",
    "        self.sza = solar_zenith_angle\n",
    "        self.r_mir = mir_radiance\n",
    "        self.r_tir = tir_radiance\n",
    "        self.output_name = output_name\n",
    "\n",
    "    def __call__(self, data:dict, *args, **kwargs):\n",
    "        sza = data[self.sza]\n",
    "        mir = data[self.r_mir]\n",
    "        tir = data[self.r_tir]\n",
    "        data[self.output_name] = self.refl_mir_calc(mir, tir, sza, sensor=kwargs['cls'].name)\n",
    "        return data\n",
    "\n",
    "    def refl_mir_calc(self, mir, tir, sza, sensor):\n",
    "        \"\"\"\n",
    "        Computes the MIR reflectance from MIR radiance and Longwave IR radiance.\n",
    "        sensor can be \"VIIRS375\" or \"VIIRS750\"\n",
    "        sza is the solar zenith angle\n",
    "        for VIIRS375, mir is band I4 and tir band I5\n",
    "        for VIIRS750, mir is band M12 and tir band M15\n",
    "        returns a matrix of MIR reflectances with the same shape as mir and tir inputs.\n",
    "        Missing values are represented by 0.\n",
    "        \"\"\"\n",
    "        lambda_M12= 3.6966\n",
    "        lambda_M15=10.7343\n",
    "        lambda_I4 = 3.7486\n",
    "        lambda_I5 = 11.4979\n",
    "\n",
    "        c1 = 1.1911e8 # [ W m-2 sr-1 (micrometer -1)-4 ]\n",
    "        c2 = 1.439e4 # [ K micrometer ]\n",
    "        E_0_mir_M12 = 11.7881 # M12 newkur_semcab\n",
    "        E_0_mir_I4= 11.2640 # I4 newkur_semcab\n",
    "\n",
    "        if sensor=='VIIRS375':\n",
    "            lambda_mir = lambda_I4\n",
    "            lambda_tir = lambda_I5\n",
    "            E_0_mir = E_0_mir_I4\n",
    "        elif sensor=='VIIRS750':\n",
    "            lambda_mir = lambda_M12\n",
    "            lambda_tir = lambda_M15\n",
    "            E_0_mir = E_0_mir_M12\n",
    "        else: raise NotImplementedError(\n",
    "            f'refl_mir_calc not implemented for {sensor}. Available options are VIIRS750 and VIIRS375.')\n",
    "\n",
    "        miu_0=np.cos((sza*np.pi)/180)\n",
    "\n",
    "        mir[mir <= 0] = np.nan\n",
    "        tir[tir <= 0] = np.nan\n",
    "\n",
    "        # Brighness temperature\n",
    "        a1 = (lambda_tir**5)\n",
    "        a = c1/(a1*tir)\n",
    "        logaritmo = np.log(a+1)\n",
    "        divisor = lambda_tir*logaritmo\n",
    "        T = (c2/divisor)\n",
    "        del a, logaritmo, divisor\n",
    "\n",
    "        # Plank function\n",
    "        divisor2 = (lambda_mir*T)\n",
    "        exponencial = np.exp(c2/divisor2)\n",
    "        b = c1*(lambda_mir**-5)\n",
    "        BT_mir = b/(exponencial-1)\n",
    "        del divisor2, exponencial, b, T\n",
    "\n",
    "        # MIR reflectance\n",
    "        c = (E_0_mir*miu_0)/np.pi\n",
    "        termo1 = (mir-BT_mir)\n",
    "        termo2 = (c-BT_mir)\n",
    "        Refl_mir = termo1/termo2\n",
    "        Refl_mir[Refl_mir <= 0] = 0\n",
    "        return Refl_mir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseDataset():\n",
    "    def __init__(self, name:str, paths:InOutPath, region:Region,\n",
    "                 times:pd.DatetimeIndex=None, bands:list=None):\n",
    "        self.paths = paths\n",
    "        self.region = region\n",
    "        self.name = name\n",
    "        self.times = times\n",
    "        self.bands = bands\n",
    "\n",
    "        if self.times is None:\n",
    "            self.times = self.find_dates()\n",
    "\n",
    "    def list_files(self, time:pd.Timestamp) -> list:\n",
    "        \"This method should return a list of filenames corresponding to the given Timestamp.\"\n",
    "        pass\n",
    "\n",
    "    def find_dates(self):\n",
    "        \"\"\"This method should return a pd.DatetimeIndex\n",
    "        with list of dates present in the data available in the input path.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def match_times(self, other, on='month'):\n",
    "        \"Set the times attribute to match the times of other dataset.\"\n",
    "        if on != 'month':\n",
    "            raise NotImplementedError('match_times is only implemented on month.')\n",
    "\n",
    "        ym_other = sorted(set([(t.year, t.month) for t in other.times]))\n",
    "        out = []\n",
    "        for t in self.times:\n",
    "            if (t.year, t.month) in ym_other:\n",
    "                out.append(t)\n",
    "        self.times = pd.DatetimeIndex(out)\n",
    "\n",
    "    def filter_times(self, year):\n",
    "        \"\"\"To select only a specific year. This can be usefull for testing and\n",
    "           for adding more new years and avoid reprocessing all the dataset.\"\"\"\n",
    "        if year is not None:\n",
    "            self.times = self.times[self.times.year == year]\n",
    "\n",
    "    def open(self, files:list, crop=None) -> dict:\n",
    "        \"\"\"This method is used to open a file or list of files for a given\n",
    "        time period and returns a dictionary with the data ready to be passed\n",
    "        to the processing functions.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict, crop=None):\n",
    "        \"Saves data in a single file for a specific timestamp in netcdf4 format.\"\n",
    "        scale_factor = 100\n",
    "        mask_value = 65535\n",
    "        tstr = time.strftime('%Y%m%d')\n",
    "        filename = f'{self.paths.dst}/{self.name}{self.region.name}_{tstr}.nc' \n",
    "        mode = \"r+\" if Path(filename).is_file() and crop is not None else \"w\"\n",
    "        with netCDF4.Dataset(filename, mode, format=\"NETCDF4\") as rootgrp:\n",
    "            if mode == \"w\":\n",
    "                latD = rootgrp.createDimension(\"lat\", self.region.height)\n",
    "                lonD = rootgrp.createDimension(\"lon\", self.region.width)\n",
    "            for k in data:\n",
    "                if mode == \"w\":\n",
    "                    a = rootgrp.createVariable(k,\"u2\",(\"lat\",\"lon\",), zlib=True)\n",
    "                    a.setncattr('scale', scale_factor)\n",
    "                    a.setncattr('mask', mask_value)\n",
    "                else: a = rootgrp[k]\n",
    "                v = data[k]*scale_factor\n",
    "                v[np.isnan(v)] = mask_value\n",
    "                assert np.sum(data[k] < 0) == 0\n",
    "                if np.sum(data[k]*scale_factor > mask_value) > 0:\n",
    "                    warn(f'Clipping data to {mask_value-1} for {k}.')\n",
    "                    v[v>mask_value] = mask_value-1\n",
    "                if crop is None:\n",
    "                    a[:] = v.astype(np.uint16)\n",
    "                else:\n",
    "                    a[crop[0]:crop[1],crop[2]:crop[3]] = v.astype(np.uint16)\n",
    "\n",
    "    def process_one(self, time:pd.Timestamp, proc_funcs:list=[], save=True, max_size=None,\n",
    "                    **proc_funcs_kwargs):\n",
    "        \"\"\"This method defines a processing pipeline consisting of opening the file\n",
    "        using the `open` method, applying each of the `proc_funcs` to the output of the previous\n",
    "        and `save` the processed data using save method.\"\"\"\n",
    "        tstr = time.strftime('%Y%m%d')\n",
    "        files = self.list_files(time)\n",
    "        #try:\n",
    "        tstr = time.strftime('%Y%m%d')\n",
    "        filename = f'{self.paths.dst}/{self.name}{self.region.name}_{tstr}.nc'\n",
    "        if not Path(filename).is_file():\n",
    "            if len(files) > 0:\n",
    "                if max_size is None:\n",
    "                    si = [[0, self.region.shape[0], 0, self.region.shape[1]]]\n",
    "                else:\n",
    "                    si = [[max(0,j*max_size),(j+1)*max_size,max(0,i*max_size),(i+1)*max_size] \n",
    "                           for i in range((self.region.shape[1]-1)//max_size+1) \n",
    "                           for j in range((self.region.shape[0]-1)//max_size+1)]\n",
    "                for c in si:\n",
    "                    #print(f'Processing crop {c}')\n",
    "                    data = self.open(files, crop=c)\n",
    "                    kwargs = {'cls': self, **proc_funcs_kwargs}\n",
    "                    for f in proc_funcs:\n",
    "                        data = f(data, time, **kwargs)\n",
    "                    if save:\n",
    "                        self.save(time, data, crop=c)\n",
    "                        del data\n",
    "                        gc.collect()\n",
    "                    else: return data\n",
    "            else:\n",
    "                warn(f'No files for {time}. Skipping to the next time.')\n",
    "        else: warn(f'Skipping files for {time}. File already exists.')\n",
    "        #except:\n",
    "        #    msg = f'Unable to process files for {time}. Check if files are corrupted. Skipping to the next time. { sys.exc_info()[0]}'\n",
    "        #    warn(msg, UserWarning)\n",
    "\n",
    "    def process_all(self, proc_funcs=[], max_workers=1, max_size=None, **proc_funcs_kwargs):\n",
    "        \"\"\"`process_all` runs `process_one` in parallel using the number of workers defined\n",
    "        by `max_workers` and passes the `proc_funcs` list to `process_one` method\"\"\"\n",
    "        process_one = partial(self.process_one, proc_funcs=proc_funcs, max_size=max_size,\n",
    "                              **proc_funcs_kwargs)\n",
    "        with ThreadPoolExecutor(max_workers) as e:\n",
    "            list(tqdm(e.map(process_one, self.times), total=len(self.times)))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f'{i}: {o}' for i, o in self.__dict__.items()]) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BaseDataset` class is a template to be used for the several types of datasets we are going to use. The methods `BaseDataset.list_files`, `BaseDataset.find_dates` and `BaseDataset.open` are just placeholders to be defined individually for each data source as they will depend on the filenames and file types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.list_files\" class=\"doc_header\"><code>BaseDataset.list_files</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.list_files</code>(**`time`**:`Timestamp`)\n",
       "\n",
       "This method should return a list of filenames corresponding to the given Timestamp."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.find_dates\" class=\"doc_header\"><code>BaseDataset.find_dates</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.find_dates</code>()\n",
       "\n",
       "This method should return a pd.DatetimeIndex\n",
       "with list of dates present in the data available in the input path."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.find_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.open\" class=\"doc_header\"><code>BaseDataset.open</code><a href=\"__main__.py#L41\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.open</code>(**`files`**:`list`, **`crop`**=*`None`*)\n",
       "\n",
       "This method is used to open a file or list of files for a given\n",
       "time period and returns a dictionary with the data ready to be passed\n",
       "to the processing functions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.open)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining methods of `BaseDataset` listed below are already defined but some may need to be redifined for particular datasets with different characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.match_times\" class=\"doc_header\"><code>BaseDataset.match_times</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.match_times</code>(**`other`**, **`on`**=*`'month'`*)\n",
       "\n",
       "Set the times attribute to match the times of other dataset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.match_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.filter_times\" class=\"doc_header\"><code>BaseDataset.filter_times</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.filter_times</code>(**`year`**)\n",
       "\n",
       "To select only a specific year. This can be usefull for testing and\n",
       "for adding more new years and avoid reprocessing all the dataset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.filter_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.save\" class=\"doc_header\"><code>BaseDataset.save</code><a href=\"__main__.py#L47\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.save</code>(**`time`**:`Timestamp`, **`data`**:`dict`, **`crop`**=*`None`*)\n",
       "\n",
       "Saves data in a single file for a specific timestamp in netcdf4 format."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.process_one\" class=\"doc_header\"><code>BaseDataset.process_one</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.process_one</code>(**`time`**:`Timestamp`, **`proc_funcs`**:`list`=*`[]`*, **`save`**=*`True`*, **`max_size`**=*`None`*, **\\*\\*`proc_funcs_kwargs`**)\n",
       "\n",
       "This method defines a processing pipeline consisting of opening the file\n",
       "using the `open` method, applying each of the `proc_funcs` to the output of the previous\n",
       "and `save` the processed data using save method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.process_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseDataset.process_all\" class=\"doc_header\"><code>BaseDataset.process_all</code><a href=\"__main__.py#L111\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseDataset.process_all</code>(**`proc_funcs`**=*`[]`*, **`max_workers`**=*`1`*, **`max_size`**=*`None`*, **\\*\\*`proc_funcs_kwargs`**)\n",
       "\n",
       "`process_all` runs `process_one` in parallel using the number of workers defined\n",
       "by `max_workers` and passes the `proc_funcs` list to `process_one` method"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseDataset.process_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def mask2nan(data:dict, *args, **kwargs) -> dict:\n",
    "    for k in data:\n",
    "        k_list = data[k]\n",
    "        for i, d in enumerate(k_list):\n",
    "            mask = d.mask\n",
    "            data[k][i] = d.data\n",
    "            data[k][i][mask] = np.nan\n",
    "    return data\n",
    "\n",
    "def nan2zero(data:dict, *args, **kwargs) -> dict:\n",
    "    for k in data:\n",
    "        data[k][np.isnan(data[k])] = 0\n",
    "    return data\n",
    "\n",
    "def group_files(files:list):\n",
    "    return pd.DataFrame({'files': files, 'ids':['.'.join(f.stem.split('.')[-4:-2]) for f in files]}\n",
    "                        ).groupby('ids').agg(lambda x : list(x)).files.values.tolist()\n",
    "\n",
    "class ViirsDataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process VIIRS 750-meter bands.\"\n",
    "    _name = None\n",
    "    def __init__(self, paths:InOutPath, region:Region,\n",
    "                 times:pd.DatetimeIndex=None, bands:list=None):\n",
    "        super().__init__(self._name, paths, region, times, bands)\n",
    "        self.times = self.check_files()\n",
    "\n",
    "    def list_files(self, time:pd.Timestamp)-> list:\n",
    "        print('Using sensor: ', sensor_id)\n",
    "        if time in self.times:\n",
    "            dayOfYear = str(time.dayofyear).zfill(3)\n",
    "            files = self.paths.src.ls(include=[f'.A{time.year}{dayOfYear}.', '.nc'])\n",
    "        return files\n",
    "\n",
    "    def check_files(self):\n",
    "        not_missing = []\n",
    "        for i, t in tqdm(enumerate(self.times), total=len(self.times)):\n",
    "            files = self.list_files(t)\n",
    "            files = ';'.join([f.stem for f in files])\n",
    "            if sum([s in files for s in self.bands]) != len(self.bands):\n",
    "                print(f'Missing files for {t}')\n",
    "            else: not_missing.append(i)\n",
    "        return self.times[not_missing]\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        pattern = r'^\\w+.A(20[0-9][0-9])([0-3][0-9][0-9])..*$'\n",
    "        times = []\n",
    "        for f in self.paths.src.ls():\n",
    "            x = re.search(pattern, f.stem)\n",
    "            if x is not None:\n",
    "                year, doy = map(x.group, [1,2])\n",
    "                times.append(pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=int(doy)-1))\n",
    "        self.times = pd.DatetimeIndex(sorted(set(times)))\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "    \n",
    "    def open_netcdf4(self, files:list, dtype=np.float32) -> dict:\n",
    "        data_dict = {}\n",
    "        for f0 in files:\n",
    "            nc_data = netCDF4.Dataset(str(f0), mode='r')\n",
    "            if 'observation_data' in nc_data.groups:\n",
    "                nc_data = nc_data.groups['observation_data']\n",
    "            elif 'geolocation_data' in nc_data.groups:\n",
    "                nc_data = nc_data.groups['geolocation_data']\n",
    "            bands = [k for k in nc_data.variables]\n",
    "            for s in list(set(bands).intersection(set(self.bands))):\n",
    "                data = nc_data[s][:].astype(dtype)\n",
    "                if s in data_dict: raise Exception('...')\n",
    "                data_dict[s] = data\n",
    "        mask = data_dict['I01'].mask | data_dict['I02'].mask | data_dict['I03'].mask | data_dict['I04'].mask | data_dict['I05'].mask\n",
    "        for k in data_dict:\n",
    "            data_dict[k].mask = mask\n",
    "        return data_dict\n",
    "    \n",
    "    def resample(self, data: dict, epsg=4326, max_distance_meter=1000, num_workers=8):\n",
    "        swath_def = SwathDefinition(lons=data['longitude'], lats=data['latitude'])\n",
    "        crs = CRS(f'EPSG:{epsg}')\n",
    "        area_def = create_area_def(crs.name, crs.to_dict(), area_extent=self.region.bbox, resolution=self.region.pixel_size)\n",
    "        valid_input_index, valid_output_index, index_array, distance_array = \\\n",
    "            kd_tree.get_neighbour_info(swath_def, area_def, max_distance_meter, neighbours=1, nprocs=num_workers)\n",
    "        for k in self.bands:\n",
    "            data[k] = kd_tree.get_sample_from_neighbour_info('nn', output_shape=region.shape, data=data[k], \n",
    "                                                   valid_input_index=valid_input_index, valid_output_index=valid_output_index,\n",
    "                                                   index_array=index_array, distance_array=distance_array, fill_value=None)\n",
    "        return data\n",
    "    \n",
    "    def process_one(self, time:pd.Timestamp, proc_funcs:list=[], save=True, replace=False, \n",
    "                    sensor_id='VJ', **proc_funcs_kwargs):\n",
    "        \"\"\"This method defines a processing pipeline consisting of opening the file\n",
    "        using the `open` method, applying each of the `proc_funcs` to the output of the previous\n",
    "        and `save` the processed data using save method.\"\"\"\n",
    "        tstr = time.strftime('%Y%m%d')\n",
    "        files = self.list_files(time, sensor_id=sensor_id)\n",
    "        files = group_files(files)\n",
    "        tstr = time.strftime('%Y%m%d')\n",
    "        filename = f'{self.paths.dst}/{self.name}{self.region.name}_{tstr}.nc'\n",
    "        if not Path(filename).is_file() or replace:\n",
    "            data_dict = {v:[] for v in self.bands}\n",
    "            for file_group in files:\n",
    "                data = self.open_netcdf4(file_group)\n",
    "                data = self.resample(data)\n",
    "                for v in self.bands:\n",
    "                    data_dict[v].append(data[v])\n",
    "            kwargs = {'cls': self, **proc_funcs_kwargs}\n",
    "            for f in proc_funcs:\n",
    "                data_dict = f(data_dict, time, **kwargs)\n",
    "            if save:\n",
    "                self.save(time, data_dict)\n",
    "            else: return data_dict\n",
    "        else: warn(f'Skipping files for {time}. File already exists.')\n",
    "            \n",
    "class Viirs375Dataset(ViirsDataset):\n",
    "    _name = 'VIIRS375'\n",
    "    \n",
    "class Viirs750Dataset(ViirsDataset):\n",
    "    _name = 'VIIRS750'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MCD64Dataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process MCD64A1 Collection 6 burned areas product.\"\n",
    "    def __init__(self, paths:InOutPath, region:Region, times:pd.DatetimeIndex=None,\n",
    "                 bands=['bafrac']):\n",
    "        super().__init__('MCD64A1C6', paths, region, times, bands)\n",
    "\n",
    "    def list_files(self, time:pd.Timestamp) -> list:\n",
    "        out = []\n",
    "        if time in self.times:\n",
    "            time = pd.Timestamp(f'{time.year}-{time.month}-01')\n",
    "            time_pattern = f'.A{time.year}{time.dayofyear:03d}.'\n",
    "            files = self.paths.src.ls(recursive=True, include=['burndate.tif', time_pattern],\n",
    "                                exclude=['.xml'])\n",
    "            # Find windows joint with region bounding box\n",
    "            for f in files:\n",
    "                data = open_tif(f)\n",
    "                if not disjoint_bounds(data.bounds, self.region.bbox):\n",
    "                    out.append(f)\n",
    "        return out\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        pattern = r'^\\w+.A(20[0-9][0-9])([0-3][0-9][0-9])..*$'\n",
    "        times = []\n",
    "        for f in self.paths.src.ls(recursive=True):\n",
    "            x = re.search(pattern, f.stem)\n",
    "            if x is not None:\n",
    "                year, doy = map(x.group, [1,2])\n",
    "                times.append(pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=int(doy)-1))\n",
    "        self.times = pd.DatetimeIndex(sorted(set(times)))\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "\n",
    "    def file_time_range(self, file) -> pd.DatetimeIndex:\n",
    "        pattern = r'^\\w+.A(20[0-9][0-9])([0-3][0-9][0-9])..*$'\n",
    "        x = re.search(pattern, file.stem)\n",
    "        year, doy = map(x.group, [1,2])\n",
    "        t0 = pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=int(doy)-1)\n",
    "        return pd.date_range(t0, periods=monthlen(t0.year, t0.month), freq='D')\n",
    "\n",
    "    def open(self, files:list) -> dict:\n",
    "        times = self.file_time_range(files[0])\n",
    "        data_dict = {'times': times}\n",
    "        out = np.zeros((len(times), *self.region.shape))\n",
    "        data = [open_tif(f) for f in files]\n",
    "        data, tfm = crop(data, self.region.bbox)\n",
    "        for i, time in enumerate(times):\n",
    "            x = (data == time.dayofyear).astype(np.int8)\n",
    "            out[i] += downsample(x, tfm, self.region.transform, self.region.shape)\n",
    "        data_dict[self.bands[0]] = out\n",
    "        return data_dict\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict):\n",
    "        v = self.bands[0]\n",
    "        for i, t in enumerate(data['times']):\n",
    "            super().save(t, {v: data[v][i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FireCCI51Dataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process FireCCI51 burned areas product.\"\n",
    "    def __init__(self, paths:InOutPath, region:Region, times:pd.DatetimeIndex=None,\n",
    "                 bands=['bafrac']):\n",
    "        super().__init__('FireCCI51', paths, region, times, bands)\n",
    "\n",
    "    def list_files(self, time:pd.Timestamp) -> list:\n",
    "        out = []\n",
    "        if time in self.times:\n",
    "            time = pd.Timestamp(f'{time.year}-{time.month}-01')\n",
    "            time_pattern = time.strftime('%Y%m%d')\n",
    "            files = self.paths.src.ls(recursive=True, include=['JD.tif', time_pattern],\n",
    "                                exclude=['.xml'])\n",
    "            # Find windows joint with region bounding box\n",
    "            for f in files:\n",
    "                data = open_tif(f)\n",
    "                if not disjoint_bounds(data.bounds, self.region.bbox):\n",
    "                    out.append(f)\n",
    "        return out\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        files = self.paths.src.ls(recursive=True, include=['JD.tif'], exclude=['.xml'])\n",
    "        self.times = pd.DatetimeIndex(sorted(set([pd.Timestamp(o.name[:8]) for o in files])))\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "\n",
    "    def file_time_range(self, file) -> pd.DatetimeIndex:\n",
    "        t0 = pd.Timestamp(file.name[:8])\n",
    "        return pd.date_range(t0, periods=monthlen(t0.year, t0.month), freq='D')\n",
    "\n",
    "    def open(self, files:list) -> dict:\n",
    "        times = self.file_time_range(files[0])\n",
    "        data_dict = {'times': times}\n",
    "        out = np.zeros((len(times), *self.region.shape))\n",
    "        data = [open_tif(f) for f in files]\n",
    "        data, tfm = crop(data, self.region.bbox)\n",
    "        for i, time in enumerate(times):\n",
    "            x = (data == time.dayofyear).astype(np.int8)\n",
    "            out[i] += downsample(x, tfm, self.region.transform, self.region.shape)\n",
    "        data_dict[self.bands[0]] = out\n",
    "        return data_dict\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict, do_compression=True):\n",
    "        v = self.bands[0]\n",
    "        for i, t in enumerate(data['times']):\n",
    "            super().save(t, {v: data[v][i]}, do_compression=do_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AusCoverDataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process AusCover burned areas product.\"\n",
    "    def __init__(self, paths:InOutPath, region:Region, times:pd.DatetimeIndex=None,\n",
    "                bands=['bafrac']):\n",
    "        super().__init__('AusCover', paths, region, times, bands)\n",
    "\n",
    "    def list_files(self, time:pd.Timestamp) -> list:\n",
    "        out = []\n",
    "        if time.year in self.times.year:\n",
    "            time = pd.Timestamp(f'{time.year}-01-01')\n",
    "            time_pattern = time.strftime('_%Y_')\n",
    "            files = self.paths.src.ls(recursive=True, include=['.tif', time_pattern],\n",
    "                                exclude=['.xml'])\n",
    "        return files\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        files = self.paths.src.ls(recursive=True, include=['.tif'], exclude=['.xml'])\n",
    "        self.times = pd.DatetimeIndex(sorted(set([pd.Timestamp(f'{o.stem[-10:-6]}-01-01')\n",
    "                                                  for o in files])))\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "\n",
    "    def file_time_range(self, file) -> pd.DatetimeIndex:\n",
    "        t0 = pd.Timestamp(f'{file.stem[-10:-6]}-01-01')\n",
    "        return pd.date_range(t0, periods=12, freq='MS')\n",
    "\n",
    "    def open(self, files:list) -> dict:\n",
    "        times = self.file_time_range(files[0])\n",
    "        data_dict = {'times': times}\n",
    "        out = np.zeros((len(times), *self.region.shape))\n",
    "        data = [open_tif(f) for f in files]\n",
    "        data = data[0]\n",
    "        crs = data.crs\n",
    "        tfm = data.transform\n",
    "        data = data.read(1)\n",
    "        for i, time in enumerate(times):\n",
    "            x = (data == time.month).astype(np.int8)\n",
    "            out[i] += downsample(x, tfm, self.region.transform,\n",
    "                                 self.region.shape, src_crs=crs)\n",
    "        data_dict[self.bands[0]] = out\n",
    "        return data_dict\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict, do_compression=True):\n",
    "        v = self.bands[0]\n",
    "        for i, t in enumerate(data['times']):\n",
    "            super().save(t, {v: data[v][i]}, do_compression=do_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MTBSDataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process MTBS burned areas product.\"\n",
    "    def __init__(self, paths:InOutPath, region:Region, times:pd.DatetimeIndex=None,\n",
    "                bands=['bafrac']):\n",
    "        super().__init__('MTBS', paths, region, times, bands)\n",
    "\n",
    "    def list_files(self, *args) -> list:\n",
    "        files = self.paths.src.ls(recursive=True, include=['.shp'], exclude=['.xml'])\n",
    "        return files\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        files = self.list_files()\n",
    "        df = open_shp(files[0])\n",
    "        self.times = pd.date_range(f'{df.Year.min()}-01-01',\n",
    "                                   f'{df.Year.max()}-12-01', freq='MS')\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "\n",
    "    def open(self, files:list) -> dict:\n",
    "        data_dict = {'times': self.times}\n",
    "        data = open_shp(files[0]).to_crs({'init': 'EPSG:4326'})\n",
    "        out = np.zeros((len(self.times), *self.region.shape))\n",
    "        R = Region(self.region.name, self.region.bbox, pixel_size=0.0003)\n",
    "        for i, time in enumerate(self.times):\n",
    "            x = data.loc[(data.Year==time.year) & (data.StartMonth==time.month)].copy()\n",
    "            x_raster = rasterize(x, region=R)\n",
    "            out[i] += downsample(x_raster, R.transform, self.region.transform,\n",
    "                                self.region.shape)\n",
    "        data_dict[self.bands[0]] = out\n",
    "        return data_dict\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict, do_compression=True):\n",
    "        v = self.bands[0]\n",
    "        for i, t in enumerate(data['times']):\n",
    "            super().save(t, {v: data[v][i]}, do_compression=do_compression)\n",
    "\n",
    "    def process_all(self, *args):\n",
    "        self.process_one(self.times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ICNFDataset(BaseDataset):\n",
    "    \"Subclass of `BaseDataset` to process ICNF burned areas product.\"\n",
    "    def __init__(self, paths:InOutPath, region:Region, times:pd.DatetimeIndex=None,\n",
    "                bands=['bafrac']):\n",
    "        super().__init__('ICNF', paths, region, times, bands)\n",
    "\n",
    "    def list_files(self, *args) -> list:\n",
    "        files = self.paths.src.ls(recursive=True, include=['.shp'], exclude=['.xml'])\n",
    "        return files\n",
    "\n",
    "    def find_dates(self, first:pd.Timestamp=None, last:pd.Timestamp=None):\n",
    "        files = self.list_files()\n",
    "        df = open_shp(files[0])\n",
    "        self.times = sorted(set([pd.Timestamp(f'{o[:-2]}01')\n",
    "                            for o in df.FIREDATE if o is not None]))\n",
    "        if first is not None:\n",
    "            self.times = self.times[self.times>=first]\n",
    "        if last is not None:\n",
    "            self.times = self.times[self.times<=last]\n",
    "        return self.times\n",
    "\n",
    "    def open(self, files:list) -> dict:\n",
    "        data_dict = {'times': self.times}\n",
    "        data = open_shp(files[0]).to_crs({'init': 'EPSG:4326'})\n",
    "        data = data.loc[~data.FIREDATE.isna()]\n",
    "        times = pd.DatetimeIndex([pd.Timestamp(o) for o in data.FIREDATE])\n",
    "        data['times'] = times\n",
    "        out = np.zeros((len(self.times), *self.region.shape))\n",
    "        R = Region(self.region.name, self.region.bbox, pixel_size=0.0003)\n",
    "        for i, time in enumerate(self.times):\n",
    "            x = data.loc[(times.year==time.year) &\n",
    "                         (times.month==time.month)].copy()\n",
    "            x_raster = rasterize(x, region=R)\n",
    "            out[i] += downsample(x_raster, R.transform, self.region.transform,\n",
    "                                self.region.shape)\n",
    "        data_dict[self.bands[0]] = out\n",
    "        return data_dict\n",
    "\n",
    "    def save(self, time:pd.Timestamp, data:dict, do_compression=True):\n",
    "        v = self.bands[0]\n",
    "        for i, t in enumerate(data['times']):\n",
    "            super().save(t, {v: data[v][i]}, do_compression=do_compression)\n",
    "\n",
    "    def process_all(self, *args):\n",
    "        self.process_one(self.times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Region2Tiles():\n",
    "    \"\"\"Crops processed dataset into 128x128 tiles to be used to train the model\"\"\"\n",
    "    def __init__(self, paths:InOutPath, input_name:str, target_name:str,\n",
    "                 regions:list=None, bands:list=None, size=128, step=100):\n",
    "        self.paths = paths\n",
    "        self.input_name = input_name\n",
    "        self.target_name = target_name\n",
    "        self.bands = bands\n",
    "        self.size = size\n",
    "        self.step = step\n",
    "        if regions is None:\n",
    "            self.regions = [o.name for o in self.paths.src.ls()]\n",
    "        else:\n",
    "            self.regions = regions\n",
    "\n",
    "        for folder in ['images', 'masks']:\n",
    "            (self.paths.dst/folder).mkdir(exist_ok=True)\n",
    "\n",
    "    def open(self, file, bands:list):\n",
    "        \"Open .mat file and select `bands`.\"\n",
    "        f = sio.loadmat(file)\n",
    "        return np.array([f[k] for k in bands]).transpose(1,2,0)\n",
    "\n",
    "    def process_one(self, file, bands, folder):\n",
    "        \"\"\"Create tiles for a `file` saving the results in `folder` for each\n",
    "         crop using `save` method\"\"\"\n",
    "        try:\n",
    "            data = self.open(file, bands)\n",
    "            rr, cc, _ = data.shape\n",
    "            for c in range(0, cc-1, self.step):\n",
    "                for r in range(0, rr-1, self.step):\n",
    "                    img = self.crop(data, r, c)\n",
    "                    if np.nansum(~np.isnan(img)) > 0:\n",
    "                        self.save(img, file, r, c, folder, bands)\n",
    "        except:\n",
    "            warn(f'Unable to process {file}.')\n",
    "\n",
    "    def process_all(self, max_workers=8, include=[]):\n",
    "        \"Run `process_one` in parallel using the number of workers given by `max_workers`\"\n",
    "        for r in self.regions:\n",
    "            print(f'Creating tiles for {r}')\n",
    "            for i, s in enumerate([self.input_name, self.target_name]):\n",
    "                files_list = self.paths.src.ls(recursive=True, include=[*include, *['.mat', r, s]])\n",
    "                folder = 'images' if s == self.input_name else 'masks'\n",
    "                bands = self.bands[i]\n",
    "                process_one = partial(self.process_one, bands=bands, folder=folder, )\n",
    "                with ThreadPoolExecutor(max_workers) as e:\n",
    "                    list(tqdm(e.map(process_one, files_list), total=len(files_list)))\n",
    "\n",
    "    def crop(self, im, r, c):\n",
    "        \"crop image into a square of size sz\"\n",
    "        sz = self.size\n",
    "        out_sz = (sz, sz, im.shape[-1])\n",
    "        rs,cs,hs = im.shape\n",
    "        tile = np.zeros(out_sz)\n",
    "        if (r+sz > rs) and (c+sz > cs):\n",
    "            tile[:rs-r, :cs-c, :] = im[r:, c:, :]\n",
    "        elif (r+sz > rs):\n",
    "            tile[:rs-r, :, :] = im[r:, c:c+sz, :]\n",
    "        elif (c+sz > cs):\n",
    "            tile[:, :cs-c, :] = im[r:r+sz ,c:, :]\n",
    "        else:\n",
    "            tile[...] = im[r:r+sz, c:c+sz, :]\n",
    "        return tile\n",
    "\n",
    "    def save(self, data, file, r, c, folder, bands):\n",
    "        \"\"\"Save `data` to `file` on `folder` selecting only the given `bands`.\n",
    "           The row and column index are included in the filename so that the large\n",
    "           scene can be reconstructed afterwards after generating the model predictions.\"\"\"\n",
    "        sio.savemat(self.paths.dst/f'{folder}/{file.stem}_{r}_{c}.mat',\n",
    "            {v: data[...,i] for i, v in enumerate(bands)}, do_compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Region2Tiles.open\" class=\"doc_header\"><code>Region2Tiles.open</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Region2Tiles.open</code>(**`file`**, **`bands`**:`list`)\n",
       "\n",
       "Open .mat file and select `bands`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Region2Tiles.save\" class=\"doc_header\"><code>Region2Tiles.save</code><a href=\"__main__.py#L67\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Region2Tiles.save</code>(**`data`**, **`file`**, **`r`**, **`c`**, **`folder`**, **`bands`**)\n",
       "\n",
       "Save `data` to `file` on `folder` selecting only the given `bands`.\n",
       "The row and column index are included in the filename so that the large\n",
       "scene can be reconstructed afterwards after generating the model predictions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Region2Tiles.crop\" class=\"doc_header\"><code>Region2Tiles.crop</code><a href=\"__main__.py#L51\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Region2Tiles.crop</code>(**`im`**, **`r`**, **`c`**)\n",
       "\n",
       "crop image into a square of size sz"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Region2Tiles.process_one\" class=\"doc_header\"><code>Region2Tiles.process_one</code><a href=\"__main__.py#L25\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Region2Tiles.process_one</code>(**`file`**, **`bands`**, **`folder`**)\n",
       "\n",
       "Create tiles for a `file` saving the results in `folder` for each\n",
       "crop using `save` method"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Region2Tiles.process_all\" class=\"doc_header\"><code>Region2Tiles.process_all</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Region2Tiles.process_all</code>(**`max_workers`**=*`8`*, **`include`**=*`[]`*)\n",
       "\n",
       "Run `process_one` in parallel using the number of workers given by `max_workers`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Region2Tiles.open)\n",
    "show_doc(Region2Tiles.save)\n",
    "show_doc(Region2Tiles.crop)\n",
    "show_doc(Region2Tiles.process_one)\n",
    "show_doc(Region2Tiles.process_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_geo.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_models.ipynb.\n",
      "Converted 04_predict.ipynb.\n",
      "Converted 04b_nrt.ipynb.\n",
      "Converted 04c_historical.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_cli.ipynb.\n",
      "Converted 07_web.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial.australia2020.ipynb.\n",
      "Converted tutorial.australia2020_100m.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
