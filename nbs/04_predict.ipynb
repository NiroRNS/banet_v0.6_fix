{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    ">This module has functions to generate the burned areas predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.vision import *\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import netCDF4\n",
    "import pdb\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from banet.core import *\n",
    "from banet.geo import Region\n",
    "from banet.models import BA_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def open_nc(fn, slice_idx=None, *args, **kwargs):\n",
    "    data = netCDF4.Dataset(fn, mode='r')\n",
    "    if slice_idx is None:\n",
    "        data = np.array([data[r][:] for r in ['Red', 'NIR', 'MIR', 'FRP']])\n",
    "    else:\n",
    "        data = np.array([data[r][slice_idx[0]:slice_idx[1], slice_idx[2]:slice_idx[3]]\n",
    "                         for r in ['Red', 'NIR', 'MIR', 'FRP']])\n",
    "    data[data == 65535] = 0\n",
    "    data = data.astype(np.float32)\n",
    "    data = data/100\n",
    "    return data\n",
    "\n",
    "def crop(im, r, c, size=128): \n",
    "    '''\n",
    "    crop image into a square of size sz, \n",
    "    '''\n",
    "    sz = size\n",
    "    out_sz = (sz, sz, im.shape[-1])\n",
    "    rs,cs,hs = im.shape\n",
    "    tile = np.zeros(out_sz)\n",
    "    if (r+sz > rs) and (c+sz > cs):\n",
    "        tile[:rs-r, :cs-c, :] = im[r:, c:, :]\n",
    "    elif (r+sz > rs):\n",
    "        tile[:rs-r, :, :] = im[r:, c:c+sz, :]\n",
    "    elif (c+sz > cs):\n",
    "        tile[:, :cs-c, :] = im[r:r+sz ,c:, :]\n",
    "    else:\n",
    "        tile[...] = im[r:r+sz, c:c+sz, :]\n",
    "    return tile\n",
    "\n",
    "def image2tiles(x, step=100):\n",
    "    tiles = []\n",
    "    rr, cc, _ = x.shape\n",
    "    for c in range(0, cc-1, step):\n",
    "        for r in range(0, rr-1, step):\n",
    "            img = crop(x, r, c)\n",
    "            tiles.append(img)\n",
    "    return np.array(tiles)\n",
    "                \n",
    "def tiles2image(tiles, image_size, size=128, step=100):\n",
    "    rr, cc, = image_size\n",
    "    sz = size\n",
    "    im = np.zeros(image_size)\n",
    "    indicator = np.zeros_like(im).astype(float)\n",
    "    k = 0\n",
    "    for c in range(0, cc-1, step):\n",
    "        for r in range(0, rr-1, step):\n",
    "            if (r+sz > rr) and (c+sz > cc):\n",
    "                im[r:, c:] += tiles[k][:rr-r, :cc-c]\n",
    "                indicator[r:, c:] += 1\n",
    "            elif (r+sz > rr):\n",
    "                im[r:, c:c+sz] += tiles[k][:rr-r, :] \n",
    "                indicator[r:, c:c+sz] += 1\n",
    "            elif (c+sz > cc):\n",
    "                im[r:r+sz ,c:] += tiles[k][:, :cc-c]\n",
    "                indicator[r:r+sz, c:] += 1\n",
    "            else:\n",
    "                im[r:r+sz, c:c+sz] += tiles[k]\n",
    "                indicator[r:r+sz, c:c+sz] += 1\n",
    "            k += 1\n",
    "    im /= indicator\n",
    "    return im\n",
    "\n",
    "def get_preds(tiles, model, weights=None):\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights)\n",
    "    mu = tensor([0.2349, 0.3548, 0.1128, 0.0016]).view(1,4,1,1,1)\n",
    "    std = tensor([0.1879, 0.1660, 0.0547, 0.0776]).view(1,4,1,1,1)\n",
    "    with torch.no_grad():\n",
    "        data = []\n",
    "        for x in tqdm(tiles):\n",
    "            model.eval()\n",
    "            if torch.cuda.is_available(): \n",
    "                model.cuda()\n",
    "            x = (x[None]-mu)/std\n",
    "            if torch.cuda.is_available(): \n",
    "                x = x.cuda()\n",
    "            out = model(x).sigmoid().float()\n",
    "            data.append(out.cpu().squeeze().numpy())\n",
    "    return np.array(data)\n",
    "\n",
    "def predict_one(iop:InOutPath, times:list, weights_files:list, region:str, threshold=0.5, \n",
    "                slice_idx=None, product='VIIRS750'):\n",
    "    fname = lambda t : iop.src/f'{product}{region}_{t.strftime(\"%Y%m%d\")}.nc'\n",
    "    files = [fname(t) for t in times]\n",
    "    im_size = open_nc(files[0], slice_idx=slice_idx).shape[1:]\n",
    "    tiles = []\n",
    "    print('Loading data and generating tiles:')\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            s = image2tiles(open_nc(file, slice_idx=slice_idx).transpose((1,2,0))).transpose((0,3,1,2))\n",
    "        except:\n",
    "            warn(f'No data for {file}')\n",
    "            s = np.zeros_like(s)\n",
    "        tiles.append(s)\n",
    "    tiles = np.array(tiles, dtype=np.float16).transpose((1, 2, 0, 3, 4))\n",
    "    tiles = torch.from_numpy(tiles).float()\n",
    "    preds_ens = []\n",
    "    for wf in weights_files:\n",
    "        if torch.cuda.is_available():\n",
    "            weights = torch.load(wf)\n",
    "        else: weights = torch.load(wf, map_location=torch.device('cpu'))\n",
    "        if 'model' in weights:\n",
    "            weights = weights['model']\n",
    "        print(f'Generating model predictions for {wf}:')\n",
    "        preds = get_preds(tiles, model=BA_Net(4, 1, 64), weights=weights)\n",
    "        preds = np.array([tiles2image(preds[:,i], im_size) for i in range(preds.shape[1])])\n",
    "        preds_ens.append(preds)\n",
    "    preds = np.array(preds_ens).mean(0)\n",
    "    return preds.astype(np.float32)\n",
    "\n",
    "def predict_time(path:InOutPath, times, weight_files:list, region:Region, \n",
    "                 threshold=0.05, save=True, max_size=2000, buffer=128, \n",
    "                 product='VIIRS750', output='data'):\n",
    "    tstart, tend = times.min(), times.max()\n",
    "    tstart = pd.Timestamp(f'{tstart.year}-{tstart.month}-01')\n",
    "    tend = pd.Timestamp(f'{tend.year}-{tend.month}-01')\n",
    "    ptimes = pd.date_range(tstart, tend, freq='MS')[1:-1]\n",
    "    ptimes_eom = pd.date_range(tstart, tend, freq='M')[1:-1]\n",
    "    preds_all = []\n",
    "    si = [[max(0,j*max_size-buffer), (j+1)*max_size+buffer, \n",
    "           max(0,i*max_size-buffer), (i+1)*max_size+buffer] \n",
    "          for i in range(region.shape[1]//max_size+1) for j in range(region.shape[0]//max_size+1)]\n",
    "\n",
    "    bas, bds = [], []\n",
    "    for i, split in progress_bar(enumerate(si), total=len(si)):\n",
    "        print(f'Split {split}')\n",
    "        preds_all = []\n",
    "        for time in ptimes:\n",
    "            time_start = pd.Timestamp((time - pd.Timedelta(days=30)).strftime('%Y-%m-15')) # Day 15, previous month\n",
    "            times = pd.date_range(time_start, periods=64, freq='D')\n",
    "            preds = predict_one(path, times, weight_files, region.name, slice_idx=split,\n",
    "                                product=product)\n",
    "            preds = preds[times.month == time.month]\n",
    "            preds_all.append(preds)\n",
    "        preds_all = np.concatenate(preds_all, axis=0)\n",
    "        ba = preds_all.sum(0)\n",
    "        ba[ba>1] = 1\n",
    "        ba[ba<threshold] = np.nan\n",
    "        bd = preds_all.argmax(0)\n",
    "        bd = bd.astype(np.float32)\n",
    "        bd[np.isnan(ba)] = np.nan\n",
    "        bas.append(ba.astype(np.float16))\n",
    "        bds.append(bd.astype(np.float16))\n",
    "    ba_all = np.zeros(region.shape, dtype=np.float16)\n",
    "    bd_all = np.zeros_like(ba_all)\n",
    "    for i, split_idx in enumerate(si):\n",
    "        ba_all[split_idx[0]:split_idx[1], split_idx[2]:split_idx[3]] = bas[i]\n",
    "        bd_all[split_idx[0]:split_idx[1], split_idx[2]:split_idx[3]] = bds[i]\n",
    "    if not save: return ba_all, bd_all\n",
    "    times = pd.date_range(ptimes[0], ptimes_eom[-1], freq='D')\n",
    "    sio.savemat(path.dst/f'{output}.mat', {'burned': ba_all, 'date': bd_all, 'times': np.array(times).astype(str)}, do_compression=True)\n",
    " \n",
    "def predict_month(iop, time, weight_files, region, threshold=0.5, save=True, slice_idx=None):\n",
    "    time_start = pd.Timestamp((time - pd.Timedelta(days=30)).strftime('%Y-%m-15')) # Day 15, previous month\n",
    "    times = pd.date_range(time_start, periods=64, freq='D')\n",
    "    preds = predict_one(iop, times, weight_files, region, threshold=threshold, slice_idx=slice_idx)\n",
    "    assert preds.shape[0] == len(times)\n",
    "    preds = preds[times.month == time.month]\n",
    "    ba = preds.sum(0)\n",
    "    bd = preds.argmax(0)\n",
    "    doy = np.asarray(pd.DatetimeIndex(times).dayofyear)\n",
    "    bd = doy[bd].astype(float)\n",
    "    bd[bd==doy[0]] = np.nan\n",
    "    bd[ba<threshold] = np.nan\n",
    "    ba[ba<threshold] = np.nan\n",
    "    ba[ba>1] = 1\n",
    "    if not save: return ba, bd\n",
    "    tstr = time.strftime('%Y%m')\n",
    "    sio.savemat(iop.dst/f'ba_{region}_{tstr}.mat', {'burned': ba, 'date': bd}, do_compression=True)\n",
    "\n",
    "def predict_nrt(path:InOutPath, time, weight_files:list, region:Region, \n",
    "                 threshold=0.1, save=True, max_size=2000, buffer=128, \n",
    "                 product='VIIRS750'):\n",
    "    times = pd.date_range(time-pd.Timedelta(days=63), time, freq='D') \n",
    "    si = [[max(0,j*max_size-buffer), (j+1)*max_size+buffer, \n",
    "           max(0,i*max_size-buffer), (i+1)*max_size+buffer] \n",
    "          for i in range(region.shape[1]//max_size+1) for j in range(region.shape[0]//max_size+1)]\n",
    "\n",
    "    bas, bds = [], []\n",
    "    for i, split in progress_bar(enumerate(si), total=len(si)):\n",
    "        print(f'Split {split}')\n",
    "        preds_all = predict_one(path, times, weight_files, region.name, slice_idx=split,\n",
    "                                product=product)\n",
    "        ba = preds_all.sum(0)\n",
    "        ba[ba>1] = 1\n",
    "        ba[ba<threshold] = np.nan\n",
    "        bd = preds_all.argmax(0)\n",
    "        bd = bd.astype(np.float32)\n",
    "        bd[np.isnan(ba)] = np.nan\n",
    "        bas.append(ba.astype(np.float16))\n",
    "        bds.append(bd.astype(np.float16))\n",
    "    ba_all = np.zeros(region.shape, dtype=np.float16)\n",
    "    bd_all = np.zeros_like(ba_all)\n",
    "    for i, split_idx in enumerate(si):\n",
    "        ba_all[split_idx[0]:split_idx[1], split_idx[2]:split_idx[3]] = bas[i]\n",
    "        bd_all[split_idx[0]:split_idx[1], split_idx[2]:split_idx[3]] = bds[i]\n",
    "    if not save: return ba_all, bd_all\n",
    "    tstr = time.strftime('%Y%m%d')\n",
    "    sio.savemat(path.dst/f'ba_{tstr}.mat', {'burned': ba_all, 'date': bd_all, 'times': np.array(times).astype(str)}, do_compression=True)\n",
    " \n",
    "def split_mask(mask, thr=0.5, thr_obj=1):\n",
    "    labled, n_objs = ndimage.label(mask > thr)\n",
    "    result = []\n",
    "    for i in range(n_objs):\n",
    "        obj = (labled == i + 1).astype(int)\n",
    "        if (obj.sum() > thr_obj): result.append(obj)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_geo.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_models.ipynb.\n",
      "Converted 04_predict.ipynb.\n",
      "Converted 04b_nrt.ipynb.\n",
      "Converted 04c_historical.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_cli.ipynb.\n",
      "Converted 07_web.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial.australia2020.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai_dev)",
   "language": "python",
   "name": "fastai_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
