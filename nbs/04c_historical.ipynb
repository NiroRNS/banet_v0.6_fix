{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp historical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Historical Dataset (2012 onwards)\n",
    ">This module has functions to generate the burned areas predictions to an extended historical period.\n",
    "\n",
    "**Note:** This module is currently being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import requests\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from nbdev.imports import test_eq\n",
    "import datetime\n",
    "from geoget.download import run_all\n",
    "from banet.core import filter_files, ls, Path, InOutPath, ProjectPath\n",
    "from banet.geo import Region\n",
    "from banet.data import *\n",
    "from banet.predict import predict_time\n",
    "Path.ls = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from nbdev.export import notebook2script\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RunManager():\n",
    "    def __init__(self, project_path:ProjectPath, region:str, times:pd.DatetimeIndex,\n",
    "                 product:str='VIIRS750', max_size=2000):\n",
    "        \"\"\"\n",
    "        project_path: banet.core.ProjectPath object\n",
    "        region: name of the region\n",
    "        times: dates for the first day of month for each month to use\n",
    "        product: VIIRS750 or VIIRS375\n",
    "        max_size: tile size to use on inference to reduce memory usage\n",
    "        \"\"\"\n",
    "        self.path    = project_path\n",
    "        self.times   = self.init_times(times)\n",
    "        self.product = product\n",
    "        self.region  = region\n",
    "        self.max_size= 2000\n",
    "        \n",
    "    def init_times(self, times):\n",
    "        tstart = times[0] - pd.Timedelta(days=15)\n",
    "        tstart = pd.Timestamp(f'{tstart.year}-{tstart.month}-01')\n",
    "        tend = times[-1] + pd.Timedelta(days=75)\n",
    "        tend = pd.Timestamp(f'{tend.year}-{tend.month}-01') - pd.Timedelta(days=1)\n",
    "        return pd.date_range(tstart, tend, freq='D')\n",
    "        \n",
    "    def check_data(self):\n",
    "        \"Check existing and missing files in dataset folder.\"\n",
    "        times = self.times\n",
    "        files, missing_files = [], []\n",
    "        for t in times:\n",
    "            tstr = t.strftime('%Y%m%d')\n",
    "            file = self.path.dataset/f'{self.product}{self.region}_{tstr}.nc'\n",
    "            if file.is_file():\n",
    "                files.append(file)\n",
    "            else:\n",
    "                missing_files.append(file)\n",
    "        return {'files': files, 'missing_files': missing_files}\n",
    "    \n",
    "    def get_download_dates(self):\n",
    "        \"Find for which new dates the files need to be downloaded.\"\n",
    "        files = self.check_data()['files']\n",
    "        if len(files) == 0: \n",
    "            start = self.times[0]\n",
    "        else:\n",
    "            start = pd.Timestamp(files[-1].stem.split('_')[-1])+pd.Timedelta(days=1)\n",
    "        start = start.strftime('%Y-%m-%d 00:00:00')\n",
    "        end = self.times[-1].strftime('%Y-%m-%d 23:59:59')\n",
    "        return start, end\n",
    "        \n",
    "    def download_viirs(self):\n",
    "        \"Download viirs data needed for the dataset.\"\n",
    "        tstart, tend = self.get_download_dates()\n",
    "        region = Region.load(f'{self.path.config}/R_{self.region}.json')\n",
    "        \n",
    "        if self.product == 'VIIRS750':\n",
    "            viirs_downloader = VIIRS750_download(region, tstart, tend)\n",
    "            viirs_downloader_list = viirs_downloader.split_times()\n",
    "            \n",
    "        elif self.product == 'VIIRS375':\n",
    "            viirs_downloader1 = VIIRS375_download(region, tstart, tend)\n",
    "            region.pixel_size = 0.1 # Angles can be interpolated later\n",
    "            viirs_downloader2 = VIIRS750_download(region, tstart, tend, \n",
    "                                bands=['SolarZenithAngle', 'SatelliteZenithAngle'])\n",
    "            viirs_downloader_list1 = viirs_downloader1.split_times()\n",
    "            viirs_downloader_list2 = viirs_downloader2.split_times()\n",
    "            viirs_downloader_list = [*viirs_downloader_list1, *viirs_downloader_list2]\n",
    "            \n",
    "        else: raise NotImplementedError(f'Not implemented for {self.product}.')\n",
    "            \n",
    "        run_all(viirs_downloader_list, self.path.ladsweb)\n",
    "        \n",
    "    def preprocess_dataset_750(self):\n",
    "        \"Apply pre-processing to the rawdata and saves results in dataset directory.\"\n",
    "        paths = InOutPath(f'{self.path.ladsweb}', f'{self.path.dataset}')\n",
    "        R = Region.load(f'{self.path.config}/R_{self.region}.json')\n",
    "        bands = ['Reflectance_M5', 'Reflectance_M7', 'Reflectance_M10', 'Radiance_M12',\n",
    "                 'Radiance_M15', 'SolarZenithAngle', 'SatelliteZenithAngle']\n",
    "        print('\\nPre-processing data...')\n",
    "        viirs = Viirs750Dataset(paths, R, bands=bands)\n",
    "        merge_tiles = MergeTiles('SatelliteZenithAngle')\n",
    "        mir_calc = MirCalc('SolarZenithAngle', 'Radiance_M12', 'Radiance_M15')\n",
    "        rename = BandsRename(['Reflectance_M5', 'Reflectance_M7'], ['Red', 'NIR'])\n",
    "        bfilter = BandsFilter(['Red', 'NIR', 'MIR'])\n",
    "        act_fires = ActiveFiresLog(f'{self.path.hotspots}/hotspots{self.region}.csv')\n",
    "        viirs.process_all(proc_funcs=[merge_tiles, mir_calc, rename, bfilter, act_fires])\n",
    "        \n",
    "    def preprocess_dataset_375(self):\n",
    "        \"Apply pre-processing to the rawdata and saves results in dataset directory.\"\n",
    "        paths = InOutPath(f'{self.path.ladsweb}', f'{self.path.dataset}')\n",
    "        R = Region.load(f'{self.path.config}/R_{self.region}.json')\n",
    "        bands = ['Reflectance_I1', 'Reflectance_I2', 'Reflectance_I3',\n",
    "                 'Radiance_I4', 'Radiance_I5', 'SolarZenithAngle', 'SatelliteZenithAngle']\n",
    "        print('\\nPre-processing data...')\n",
    "        viirs = Viirs375Dataset(paths, R, bands=bands)\n",
    "        interpAng = InterpolateAngles(R.new(pixel_size=0.1), R, \n",
    "                      ['SolarZenithAngle', 'SatelliteZenithAngle'])\n",
    "        merge_tiles = MergeTiles('SatelliteZenithAngle')\n",
    "        mir_calc = MirCalc('SolarZenithAngle', 'Radiance_I4', 'Radiance_I5')\n",
    "        rename = BandsRename(['Reflectance_I1', 'Reflectance_I2'], ['Red', 'NIR'])\n",
    "        bfilter = BandsFilter(['Red', 'NIR', 'MIR'])\n",
    "        act_fires = ActiveFiresLog(f'{self.path.hotspots}/hotspots{self.region}.csv')\n",
    "        viirs.process_all(proc_funcs=[interpAng, BandsAssertShape(), merge_tiles, \n",
    "                                      mir_calc, rename, bfilter, act_fires])\n",
    "        \n",
    "    def preprocess_dataset(self):\n",
    "        if self.product == 'VIIRS750':\n",
    "            self.preprocess_dataset_750()\n",
    "        elif self.product == 'VIIRS375':\n",
    "            self.preprocess_dataset_375()\n",
    "        else: raise NotImplementedError(f'Not implemented for {self.product}.')\n",
    "        \n",
    "    def init_model_weights(self, weight_files:list):\n",
    "        \"Downloads model weights if they don't exist yet on config directory.\"\n",
    "        local_files = []\n",
    "        for w in weight_files:\n",
    "            file_save = self.path.config/w\n",
    "            if not file_save.is_file():\n",
    "                print(f'Downloading model weights {w}')\n",
    "                url = f'https://github.com/mnpinto/banet_weights/raw/master/model/{w}'\n",
    "                file = requests.get(url)\n",
    "                open(str(file_save), 'wb').write(file.content)\n",
    "            local_files.append(file_save)\n",
    "        return local_files\n",
    "    \n",
    "    def get_preds(self, weight_files:list, threshold=0.5, save=True, max_size=2000,\n",
    "                  filename='data'):\n",
    "        \"Computes BA-Net predictions ensembling the models in the weight_files list.\"\n",
    "        local_files = self.init_model_weights(weight_files)\n",
    "        iop = InOutPath(self.path.dataset, self.path.outputs, mkdir=False)\n",
    "        region = Region.load(f'{self.path.config}/R_{self.region}.json')\n",
    "        predict_time(iop, self.times, local_files, region, threshold=threshold,\n",
    "                     save=save, max_size=max_size, product=self.product, output=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RunManager.preprocess_dataset\" class=\"doc_header\"><code>RunManager.preprocess_dataset</code><a href=\"__main__.py#L104\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RunManager.preprocess_dataset</code>()\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RunManager.init_model_weights\" class=\"doc_header\"><code>RunManager.init_model_weights</code><a href=\"__main__.py#L111\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RunManager.init_model_weights</code>(**`weight_files`**:`list`)\n",
       "\n",
       "Downloads model weights if they don't exist yet on config directory."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RunManager.get_preds\" class=\"doc_header\"><code>RunManager.get_preds</code><a href=\"__main__.py#L124\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RunManager.get_preds</code>(**`weight_files`**:`list`, **`threshold`**=*`0.5`*, **`save`**=*`True`*, **`max_size`**=*`2000`*, **`filename`**=*`'data'`*)\n",
       "\n",
       "Computes BA-Net predictions ensembling the models in the weight_files list."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RunManager.preprocess_dataset)\n",
    "show_doc(RunManager.init_model_weights)\n",
    "show_doc(RunManager.get_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all processes looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "region = 'BR'\n",
    "paths = ProjectPath('../hide/historical_test')\n",
    "weight_files = ['banetv0.20-val2017-fold0.pth']\n",
    "\n",
    "times = pd.date_range('2015-08-01', '2015-09-01', freq='MS')\n",
    "# Save R_{region}.json file in config folder\n",
    "manager = RunManager(paths, region, times, product='VIIRS375')\n",
    "manager.download_viirs()\n",
    "# Save hotspots{region}.json file in hotspots folder\n",
    "manager.preprocess_dataset()\n",
    "manager.get_preds(weight_files, threshold=0.01, filename='ba100m')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "region = 'PT'\n",
    "paths = ProjectPath('../hide/historical_test')\n",
    "weight_files = ['banetv0.20-val2017-fold0.pth']\n",
    "times = pd.date_range('2017-06-01', '2017-10-01', freq='MS')\n",
    "Region(region, [-10, 36, -6, 44], 0.001).export(paths.config/f'R_{region}.json')\n",
    "\n",
    "manager = RunManager(paths, region, times, product='VIIRS375')\n",
    "manager.download_viirs()\n",
    "# Save hotspots{region}.json file in hotspots folder\n",
    "manager.preprocess_dataset()\n",
    "manager.get_preds(weight_files, threshold=0.01, filename=f'ba100m_{region}{times[0].year}')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#hide \n",
    "\n",
    "#request    : 1h10\n",
    "#download   : 5h20\n",
    "#preprocess : 1h25\n",
    "#model      : 1h30\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "paths = ProjectPath('../hide/historical_test')\n",
    "data = sio.loadmat(paths.outputs/'banet100m_PT2017.mat')\n",
    "\n",
    "burned = data['burned']\n",
    "I = burned < 0.5\n",
    "burned[I] = np.nan\n",
    "date = data['date']\n",
    "date[I] = np.nan\n",
    "\n",
    "times = pd.date_range('2017-06-01', '2017-10-31')\n",
    "\n",
    "sio.savemat(paths.outputs/'banet100m_PT2017.mat', \n",
    "            {'burned': burned, 'date': date, 'times': times.astype(str).tolist()},\n",
    "            do_compression=True)\n",
    "\n",
    "plt.figure(figsize=(24,24), dpi=300)\n",
    "plt.imshow(date)\n",
    "\n",
    "data['times'].shape\n",
    "\n",
    "data1000_6 = sio.loadmat('/media/mnpinto/Data1/BaNet_1000/BaNet_PT201706-0.mat')\n",
    "data1000_7 = sio.loadmat('/media/mnpinto/Data1/BaNet_1000/BaNet_PT201707-0.mat')\n",
    "data1000_8 = sio.loadmat('/media/mnpinto/Data1/BaNet_1000/BaNet_PT201708-0.mat')\n",
    "data1000_9 = sio.loadmat('/media/mnpinto/Data1/BaNet_1000/BaNet_PT201709-0.mat')\n",
    "data1000_10 = sio.loadmat('/media/mnpinto/Data1/BaNet_1000/BaNet_PT201710-0.mat')\n",
    "\n",
    "data1000 = np.concatenate(\n",
    "    (data1000_6['probs'], data1000_7['probs'], data1000_8['probs'],\n",
    "     data1000_9['probs'], data1000_10['probs']), axis=0)\n",
    "del data1000_6, data1000_7, data1000_8, data1000_9, data1000_10\n",
    "\n",
    "burned1000 = data1000.sum(0)\n",
    "burned1000[burned1000>1] = 1\n",
    "I = burned1000 < 0.5\n",
    "burned1000[I] = np.nan\n",
    "date1000 = data1000.argmax(0).astype(np.float32)\n",
    "date1000[I] = np.nan\n",
    "\n",
    "plt.figure(figsize=(24,24), dpi=300)\n",
    "plt.imshow(date1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_geo.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_models.ipynb.\n",
      "Converted 04_predict.ipynb.\n",
      "Converted 04b_nrt.ipynb.\n",
      "Converted 04c_historical.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_cli.ipynb.\n",
      "Converted 07_web.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial.australia2020.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai_dev)",
   "language": "python",
   "name": "fastai_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
