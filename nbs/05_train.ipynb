{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    ">This module include classes and functions to train BA-Net. **Note: This module uses fastai version 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "\n",
    "from banet.models import BA_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SampleEpisode(Sampler):\n",
    "    def __init__(self, data_source, n_episodes, sequence_len, n_sequences, info_df, nburned=100):\n",
    "        self.ds, self.epoch_size = data_source, n_episodes\n",
    "        self.sequence_len, self.n_sequences = sequence_len, n_sequences\n",
    "        self._epochs = []\n",
    "        self.df = info_df\n",
    "        self.nburned = nburned\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size*self.sequence_len*self.n_sequences\n",
    "\n",
    "    def __iter__(self): return iter(self.get_epoch())\n",
    "\n",
    "    def get_epoch(self):\n",
    "        \"\"\"Get indices for one epoch of size epoch_size\"\"\"\n",
    "        idx = []\n",
    "        for n in range(self.epoch_size):\n",
    "            idx = [*idx, *self.get_batch()]\n",
    "        return idx\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"Get indices for one mini-batch\"\"\"\n",
    "        idx = []\n",
    "        n = 0\n",
    "        while n < self.n_sequences:\n",
    "            k = np.random.choice(self.df.loc[self.df.ba>self.nburned].index, size=1, replace=False)[0]\n",
    "            s = self.random_sample(k)\n",
    "            if s is not None:\n",
    "                idx = [*idx, *s]\n",
    "                n += 1\n",
    "        return idx\n",
    "\n",
    "    def random_sample(self, k):\n",
    "        \"\"\"Random samples are n-way k-shot\"\"\"\n",
    "        idx = []\n",
    "        condition = ((self.df.name == self.df.loc[k, 'name']) &\n",
    "            (self.df.time == self.df.loc[k, 'time'] + pd.Timedelta(days=self.sequence_len)) &\n",
    "            (self.df.r == self.df.loc[k, 'r']) &\n",
    "            (self.df.c == self.df.loc[k, 'c']))\n",
    "        where = self.df.loc[condition].index.values\n",
    "        if len(where) == 0:\n",
    "            idx = None\n",
    "        else:\n",
    "            times = pd.date_range(self.df.loc[k-self.sequence_len//2, 'time'], periods=2*self.sequence_len, freq='D')\n",
    "            condition = ((self.df.name == self.df.loc[k, 'name']) &\n",
    "                (self.df.time.isin(times)) &\n",
    "                (self.df.r == self.df.loc[k, 'r']) &\n",
    "                (self.df.c == self.df.loc[k, 'c']))\n",
    "            where = self.df.loc[condition].sort_values(by='time').index.values\n",
    "            idx = where[:self.sequence_len]\n",
    "            if len(idx) != self.sequence_len: idx = None\n",
    "        return idx\n",
    "\n",
    "class ImageSequence(LearnerCallback):\n",
    "    def __init__(self, learn, sequence_len=64, n_sequences=1):\n",
    "        super().__init__(learn)\n",
    "        self.sequence_len = sequence_len\n",
    "        self.n_sequences = n_sequences\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, epoch, iteration, **kwargs):\n",
    "        bs, ch, sz1, sz2 = last_input.size()\n",
    "        last_input = last_input.view(self.sequence_len, self.n_sequences, ch, sz1, sz2).permute(1, 2, 0, 3, 4)\n",
    "        last_target = last_target.view(self.sequence_len, self.n_sequences, 1, sz1, sz2).permute(1, 2, 0, 3, 4)#.max(2)[0]\n",
    "        return {'last_input': last_input, 'last_target': last_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_y_fn(file, satellite='VIIRS750', target_product='MCD64A1C6'):\n",
    "    f = str(Path(str(file))).replace('images', 'masks')\n",
    "    f = f.replace(satellite, target_product)\n",
    "    return f\n",
    "\n",
    "def open_mat(fn, *args, **kwargs):\n",
    "    data = sio.loadmat(fn)\n",
    "    data = np.array([data[r] for r in ['Red', 'NIR', 'MIR', 'FRP']])\n",
    "    data[np.isnan(data)] = 0\n",
    "    data[-1, ...] = np.log1p(data[-1,...])\n",
    "    data[np.isnan(data)] = 0\n",
    "    data = torch.from_numpy(data).float()\n",
    "    return Image(data)\n",
    "\n",
    "def open_mask(fn, *args, **kwargs):\n",
    "    data = sio.loadmat(fn)['bafrac']\n",
    "    data[np.isnan(data)] = 0\n",
    "    data = torch.from_numpy(data).float()\n",
    "    return Image(data.view(-1, data.size()[0], data.size()[1]))\n",
    "\n",
    "def set_info_df(items_list, satellite='VIIRS750', target_product='MCD64A1C6'):\n",
    "    names, dates = [], []\n",
    "    rs, cs = [], []\n",
    "    for o in items_list:\n",
    "        name, date, r,  c = Path(o).stem.split('_')\n",
    "        date = pd.Timestamp(date)\n",
    "        names.append(name)\n",
    "        dates.append(date)\n",
    "        rs.append(r)\n",
    "        cs.append(c)\n",
    "    ba = [open_mask(get_y_fn(str(o), satellite=satellite, target_product=target_product)\n",
    "                   ).data.sum().item() for o in progress_bar(items_list)]\n",
    "    return pd.DataFrame({'name': names, 'time': dates, 'r':rs, 'c':cs, 'ba':ba})\n",
    "\n",
    "class SegLabelListCustom(SegmentationLabelList):\n",
    "    def open(self, fn): return open_mask(fn, div=True)\n",
    "\n",
    "class SegItemListCustom(ImageList):\n",
    "    _label_cls = SegLabelListCustom\n",
    "    def open(self, fn): return open_mat(fn)\n",
    "\n",
    "def _cutout(x, n_holes:uniform_int=1, length:uniform_int=40):\n",
    "    \"Cut out `n_holes` number of square holes of size `length` in image at random locations.\"\n",
    "    h,w = x.shape[1:]\n",
    "    for n in range(n_holes):\n",
    "        h_y = np.random.randint(0, h)\n",
    "        h_x = np.random.randint(0, w)\n",
    "        y1 = int(np.clip(h_y - length / 2, 0, h))\n",
    "        y2 = int(np.clip(h_y + length / 2, 0, h))\n",
    "        x1 = int(np.clip(h_x - length / 2, 0, w))\n",
    "        x2 = int(np.clip(h_x + length / 2, 0, w))\n",
    "        #x[:2, y1:y2, x1:x2] = 1\n",
    "        x[-1, y1:y2, x1:x2] = 0\n",
    "    return x\n",
    "\n",
    "cutout = TfmPixel(_cutout, order=20, )\n",
    "\n",
    "def _cutout2(x, n_holes:uniform_int=1, length:uniform_int=40):\n",
    "    \"Cut out `n_holes` number of square holes of size `length` in image at random locations.\"\n",
    "    h,w = x.shape[1:]\n",
    "    h_y = np.random.randint(0, h)\n",
    "    h_x = np.random.randint(0, w)\n",
    "    y1 = int(np.clip(h_y - length / 2, 0, h))\n",
    "    y2 = int(np.clip(h_y + length / 2, 0, h))\n",
    "    x1 = int(np.clip(h_x - length / 2, 0, w))\n",
    "    x2 = int(np.clip(h_x + length / 2, 0, w))\n",
    "    x[0, y1:y2, x1:x2] = torch.rand(1)\n",
    "    x[1, y1:y2, x1:x2] = torch.rand(1)\n",
    "    x[2, y1:y2, x1:x2] = torch.rand(1)\n",
    "    return x\n",
    "\n",
    "cutout2 = TfmPixel(_cutout2, order=20)\n",
    "\n",
    "class BCE(Module):\n",
    "    \"Binary Cross Entropy loss.\"\n",
    "    def forward(self, x, y):\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "        return 100*bce(x.view(x.size()[0],-1),y.view(y.size()[0], -1))\n",
    "\n",
    "def accuracy(input:Tensor, targs:Tensor, thr:int=0.5)->Rank0Tensor:\n",
    "    \"Compute accuracy with `targs` when `input` is bs * n_classes.\"\n",
    "    input = (input.sigmoid()>thr).long()\n",
    "    targs = (targs>thr).long()\n",
    "    return (input==targs).float().mean()\n",
    "\n",
    "def dice2d(pred, targs, thr=0.5):\n",
    "    pred = pred.squeeze()\n",
    "    targs = targs.squeeze().sum(0)\n",
    "    pred = (pred.sigmoid().sum(0)>thr).float()\n",
    "    targs = (targs>thr).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "\n",
    "def mae(pred, targs, thr=0.5):\n",
    "    a = pred.squeeze().sigmoid().sum(0)>thr\n",
    "    pred = pred.squeeze().max(0)[1]\n",
    "    targs = targs.squeeze().max(0)[1]\n",
    "    pred = pred[a.byte()]\n",
    "    targs = targs[a.byte()]\n",
    "    return (pred-targs).abs().float().mean()\n",
    "\n",
    "def train_model(val_year, r_fold, path, model_path, n_epochs=8, lr=1e-2, nburned=10, n_episodes_train=2000,\n",
    "        n_episodes_valid=100, sequence_len=64, n_sequences=1, do_cutout=True, model_arch=None,\n",
    "        pretrained_weights=None, satellite='VIIRS750', target_product='MCD64A1C6',\n",
    "        get_learner=False):\n",
    "    path_img = path/'images'\n",
    "    train_files = sorted([f.name for f in path_img.iterdir()])\n",
    "    times = pd.DatetimeIndex([pd.Timestamp(t.split('_')[1]) for t in train_files])\n",
    "\n",
    "    train_df = pd.DataFrame({'times': times, 'ID': train_files})\n",
    "    valid_idx = train_df.loc[train_df.times.dt.year == val_year].index.values\n",
    "\n",
    "    if do_cutout:\n",
    "        tfms = get_transforms(do_flip=False, max_zoom=0, max_warp=0, max_rotate=0,\n",
    "                          xtra_tfms=[cutout(n_holes=(1, 5), length=(5, 50), p=0.5),\n",
    "                                     cutout2(n_holes=(1, 5), length=(5, 50), p=0.5)])\n",
    "    else:\n",
    "        tfms = get_transforms(do_flip=False, max_zoom=0, max_warp=0, max_rotate=0)\n",
    "\n",
    "    data = (SegItemListCustom.from_df(train_df, path, cols='ID', folder='images')\n",
    "        .split_by_idx(valid_idx)\n",
    "        .label_from_func(\n",
    "            partial(get_y_fn, satellite=satellite, target_product=target_product),\n",
    "            classes=['Burned'])\n",
    "        .transform(tfms, size=128, tfm_y=False))\n",
    "\n",
    "    info_train_df = set_info_df(data.train.items, \n",
    "                                satellite=satellite, target_product=target_product)\n",
    "    info_valid_df = set_info_df(data.valid.items, \n",
    "                                satellite=satellite, target_product=target_product)\n",
    "\n",
    "    bs = sequence_len*n_sequences\n",
    "    train_dl = DataLoader(\n",
    "        data.train,\n",
    "        batch_size=bs,\n",
    "        sampler=SampleEpisode(data.train, n_episodes=n_episodes_train,\n",
    "                              sequence_len=sequence_len, n_sequences=n_sequences,\n",
    "                              info_df=info_train_df, nburned=nburned))\n",
    "    valid_dl = DataLoader(\n",
    "        data.valid,\n",
    "        batch_size=bs,\n",
    "        sampler=SampleEpisode(data.valid, n_episodes=n_episodes_valid,\n",
    "                              sequence_len=sequence_len, n_sequences=n_sequences,\n",
    "                              info_df=info_valid_df, nburned=nburned))\n",
    "\n",
    "    databunch = ImageDataBunch(train_dl, valid_dl, path='.')\n",
    "    databunch = databunch.normalize([tensor([0.2349, 0.3548, 0.1128, 0.0016]),\n",
    "                                     tensor([0.1879, 0.1660, 0.0547, 0.0776])])\n",
    "\n",
    "    if model_arch is None:\n",
    "        model = BA_Net(4, 1, sequence_len)\n",
    "    else:\n",
    "        model = model_arch(4, 1, sequence_len)\n",
    "\n",
    "    if pretrained_weights is not None:\n",
    "        print(f'Loading pretrained_weights from {pretrained_weights}\\n')\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(pretrained_weights)['model'])\n",
    "        else: \n",
    "            model.load_state_dict(\n",
    "                torch.load(pretrained_weights, map_location=torch.device('cpu'))['model'])\n",
    "    learn = Learner(databunch, model, callback_fns=[\n",
    "        partial(ImageSequence, sequence_len=sequence_len, n_sequences=n_sequences)],\n",
    "        loss_func=BCE(), wd=1e-2, metrics=[accuracy, dice2d, mae])\n",
    "    learn.clip_grad = 1\n",
    "    if get_learner: return learn\n",
    "    print('Starting traning loop\\n')\n",
    "    learn.fit_one_cycle(n_epochs, lr)\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "    torch.save(learn.model.state_dict(), model_path/f'banet-val{val_year}-fold{r_fold}-test.pth')\n",
    "    print(f'Completed! banet-val{val_year}-fold{r_fold}-test.pth saved to {model_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_geo.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 03_models.ipynb.\n",
      "Converted 04_predict.ipynb.\n",
      "Converted 04b_nrt.ipynb.\n",
      "Converted 04c_historical.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_cli.ipynb.\n",
      "Converted 07_web.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial.australia2020.ipynb.\n",
      "Converted tutorial.australia2020_100m.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai_dev)",
   "language": "python",
   "name": "fastai_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
